# DreamerV4 Configuration for MineRL Dataset
# Based on: "Training Agents Inside of Scalable World Models"

# Experiment settings
experiment:
  name: "dreamerv4_minerl"
  seed: 42
  device: "cuda"
  log_dir: "logs"
  checkpoint_dir: "checkpoints"

# Data settings
data:
  path: "data/mineRL_extracted"  # Path to extracted MineRL dataset (after running extract_minerl_frames.py)
  sequence_length: 16
  # MineRL POV camera: 64×64×3 (RGB) - standard MineRL resolution
  image_height: 64
  image_width: 64
  in_channels: 3
  batch_size: 16
  num_workers: 4
  frame_skip: 1
  # Frames are already extracted at 64x64, no resize needed
  resize_to: null

# Tokenizer settings (shared transformer architecture)
tokenizer:
  patch_size: 8  # 8×8 patches: (64/8) × (64/8) = 8 × 8 = 64 patches/frame
  embed_dim: 256
  latent_dim: 32
  num_latent_tokens: 16  # Compressed from 64 patches (8×8)
  depth: 6
  num_heads: 8
  dropout: 0.0
  num_registers: 4
  mask_ratio: 0.75

# Dynamics model settings (same transformer architecture as tokenizer)
dynamics:
  # Inherits embed_dim, depth, num_heads from tokenizer
  max_shortcut_steps: 6  # K in paper (2^K = 64 max steps)
  # MineRL action space (verified from actual dataset):
  # - 8 binary keyboard: forward, back, left, right, jump, sneak, sprint, attack
  # - Camera: continuous 2D (pitch, yaw) → discretize to 121 classes (11×11 bins) with foveated discretization
  #   Range: [-150.0, 75.0] (based on actual dataset: pitch [-62.25, 60.60], yaw [-146.85, 69.75])
  # - 5 discrete string actions:
  #   * craft: 5 values (crafting_table, planks, stick, torch, none)
  #   * equip: 5 values (iron_pickaxe, stone_pickaxe, wooden_axe, wooden_pickaxe, none)
  #   * nearbyCraft: 6 values (furnace, iron_pickaxe, stone_pickaxe, wooden_axe, wooden_pickaxe, none)
  #   * nearbySmelt: 2 values (iron_ingot, none)
  #   * place: 7 values (cobblestone, crafting_table, dirt, furnace, stone, torch, none)
  # Total discrete combinations: 5×5×6×2×7 = 2,100 (but typically only ~15-20 are used in practice)
  # For DreamerV4: use multi-discrete format (as per Training.md)
  # - Keyboard: 8 binary distributions (independent Bernoulli)
  # - Mouse/Camera: Categorical with foveated discretization (121 classes)
  action_space_type: "multi_discrete"  # "categorical" or "multi_discrete"
  # If categorical: combines into single 144-action space (backward compatibility)
  # If multi_discrete: keyboard (8 binary) + camera (121 categorical)
  num_discrete_actions: 144  # Only used if action_space_type == "categorical"
  num_registers: 4

# Agent heads settings
heads:
  hidden_dim: 256
  num_layers: 2
  num_bins: 255  # For distributional value/reward
  # Action space for MineRL (verified from actual dataset)
  num_actions: 144  # Total discrete actions (8 binary keyboard + 121 camera + 15 discrete combinations)
  # Breakdown:
  # - 8 binary keyboard actions (forward, back, left, right, jump, sneak, sprint, attack)
  # - 121 camera actions (11×11 discretization of pitch/yaw)
  # - 15 discrete action combinations (from craft, equip, nearbyCraft, nearbySmelt, place)
  # If using multi-discrete:
  # keyboard_binary: 8  # 8 independent Bernoulli distributions
  # camera_discrete: 121  # 121-class categorical (11×11 bins for pitch×yaw)

# Training settings
training:
  # Phase 1: World Model Pretraining
  phase1:
    epochs: 100
    learning_rate: 3.0e-4
    weight_decay: 0.01
    warmup_steps: 1000
    max_grad_norm: 1.0
    lpips_weight: 0.2
    save_every: 10

  # Phase 2: Agent Finetuning
  phase2:
    epochs: 50
    learning_rate: 1.0e-4
    weight_decay: 0.01
    warmup_steps: 500
    max_grad_norm: 1.0
    reward_weight: 1.0
    freeze_transformer: true
    save_every: 5
    # Focal loss for sparse MineRL rewards
    use_focal_loss: true
    focal_gamma: 2.0
    focal_alpha: 0.25

  # Phase 3: Imagination Training with PMPO
  phase3:
    epochs: 100
    batch_size: 16
    
    # Imagination rollout settings
    imagination_horizon: 15  # H - rollout length
    num_denoising_steps: 4   # K - shortcut sampling steps
    
    # RL hyperparameters
    # For MineRL (sparse rewards, long horizons):
    # - High discount (0.997) ensures rewards 1000+ steps away still contribute
    # - Lambda (0.95) balances bias-variance tradeoff for TD(λ)
    # - For very sparse rewards, consider lambda: 0.98-0.99 to better propagate signals
    discount: 0.997          # γ - high for long-horizon tasks (obtaining diamonds can take 1000+ steps)
    lambda: 0.95             # λ - TD(λ) trace decay (standard value, can increase to 0.98 for sparser rewards)
    
    # EMA target network settings (Section 4.4)
    value_ema_decay: 0.999   # EMA decay rate for value target network (higher = slower updates)
    
    # PMPO (Equation 11)
    pmpo_alpha: 0.5          # α - weight for positive advantage updates
    pmpo_beta_kl: 0.3        # β - KL regularization to prior (paper default)
    entropy_coef: 0.003      # Entropy bonus for exploration
    advantage_bins: 16       # Number of bins for advantage binning
    # Percentile-based binning for sparse MineRL rewards
    use_percentile_binning: true
    percentile_threshold: 10.0  # Top/bottom 10% for D+/D-
    
    # Learning rates
    policy_lr: 3.0e-5        # Slower than world model
    value_lr: 1.0e-4         # Critic learning rate
    
    # Loss weights
    value_loss_scale: 0.5    # Scale for value loss
    
    # Training
    grad_clip: 1.0
    save_every: 10
    num_workers: 4

# Optimizer settings
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler settings
scheduler:
  type: "cosine"
  min_lr: 1.0e-6

# Logging settings
logging:
  log_every: 100
  eval_every: 1000
  use_wandb: false
  wandb_project: "dreamerv4"
