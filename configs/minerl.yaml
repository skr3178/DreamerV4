# DreamerV4 Configuration for MineRL Dataset
# Based on: "Training Agents Inside of Scalable World Models"

# Experiment settings
experiment:
  name: "dreamerv4_minerl"
  seed: 42
  device: "cuda"
  log_dir: "logs"
  checkpoint_dir: "checkpoints"

# Data settings
data:
  path: "data/mineRL_extracted"  # Path to extracted MineRL dataset (after running extract_minerl_frames.py)
  sequence_length: 16
  # MineRL POV camera: 64×64×3 (RGB) - standard MineRL resolution
  image_height: 64
  image_width: 64
  in_channels: 3
  batch_size: 16
  num_workers: 4
  frame_skip: 1
  # Frames are already extracted at 64x64, no resize needed
  resize_to: null

# Tokenizer settings (shared transformer architecture)
tokenizer:
  patch_size: 8  # 8×8 patches: (64/8) × (64/8) = 8 × 8 = 64 patches/frame
  embed_dim: 256
  latent_dim: 32
  num_latent_tokens: 16  # Compressed from 64 patches (8×8)
  depth: 6
  num_heads: 8
  dropout: 0.0
  num_registers: 4
  mask_ratio: 0.75

# Dynamics model settings (same transformer architecture as tokenizer)
dynamics:
  # Inherits embed_dim, depth, num_heads from tokenizer
  max_shortcut_steps: 6  # K in paper (2^K = 64 max steps)
  # MineRL action space (from inspection):
  # - 8 binary keyboard: forward, left, back, right, jump, sneak, sprint, attack
  # - Camera: continuous 2D (pitch, yaw) → discretize to 121 classes (11×11 bins)
  # - Additional discrete: place, equip, craft, nearbyCraft, nearbySmelt (varies by env)
  # For DreamerV4: combine into single categorical space
  num_discrete_actions: 144  # 8 binary + 121 camera + 15 additional = ~144 total
  # Alternative: use multi-discrete (8 binary + 121 categorical + others)
  action_space_type: "categorical"  # "categorical" or "multi_discrete"
  num_registers: 4

# Agent heads settings
heads:
  hidden_dim: 256
  num_layers: 2
  num_bins: 255  # For distributional value/reward
  # Action space for MineRL (from inspection)
  num_actions: 144  # Total discrete actions (8 binary keyboard + 121 camera + 15 additional)
  # If using multi-discrete:
  # keyboard_binary: 8  # 8 independent Bernoulli distributions
  # camera_discrete: 121  # 121-class categorical (11×11 bins for pitch×yaw)

# Training settings
training:
  # Phase 1: World Model Pretraining
  phase1:
    epochs: 100
    learning_rate: 3.0e-4
    weight_decay: 0.01
    warmup_steps: 1000
    max_grad_norm: 1.0
    lpips_weight: 0.2
    save_every: 10

  # Phase 2: Agent Finetuning
  phase2:
    epochs: 50
    learning_rate: 1.0e-4
    weight_decay: 0.01
    warmup_steps: 500
    max_grad_norm: 1.0
    reward_weight: 1.0
    freeze_transformer: true
    save_every: 5

  # Phase 3: Imagination Training with PMPO
  phase3:
    epochs: 100
    batch_size: 16
    
    # Imagination rollout settings
    imagination_horizon: 15  # H - rollout length
    num_denoising_steps: 4   # K - shortcut sampling steps
    
    # RL hyperparameters
    discount: 0.997          # γ - high for long-horizon tasks
    lambda: 0.95             # λ - TD(λ) trace decay
    
    # PMPO (Equation 11)
    pmpo_alpha: 0.5          # α - weight for positive advantage updates
    pmpo_beta_kl: 0.3        # β - KL regularization to prior (paper default)
    entropy_coef: 0.003      # Entropy bonus for exploration
    advantage_bins: 16       # Number of bins for advantage binning
    
    # Learning rates
    policy_lr: 3.0e-5        # Slower than world model
    value_lr: 1.0e-4         # Critic learning rate
    
    # Loss weights
    value_loss_scale: 0.5    # Scale for value loss
    
    # Training
    grad_clip: 1.0
    save_every: 10
    num_workers: 4

# Optimizer settings
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler settings
scheduler:
  type: "cosine"
  min_lr: 1.0e-6

# Logging settings
logging:
  log_every: 100
  eval_every: 1000
  use_wandb: false
  wandb_project: "dreamerv4"
