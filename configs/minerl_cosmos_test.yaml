# Temporary test config - quick test with minimal training
# Based on minerl_cosmos.yaml

experiment:
  name: "dreamerv4_cosmos_test"
  seed: 42
  device: "cuda"
  log_dir: "logs"
  checkpoint_dir: "checkpoints"

cosmos_tokenizer:
  enabled: true
  checkpoint_path: "cosmos_tokenizer/CV8x8x8"
  model_variant: "CV8x8x8"
  dtype: "bfloat16"
  input_resolution: 256
  pool_tokens: 16

data:
  path: "data/mineRL_subset"
  sequence_length: 32
  image_height: 64
  image_width: 64
  in_channels: 3
  batch_size: 4
  num_workers: 4
  frame_skip: 1
  resize_to: null
  max_episodes: 5  # Limit dataset size for quick testing

tokenizer:
  num_latent_tokens: 16
  latent_dim: 16
  patch_size: 8
  embed_dim: 256
  depth: 6
  num_heads: 8
  dropout: 0.0
  num_registers: 4
  mask_ratio: 0.75

dynamics:
  embed_dim: 256
  num_layers: 12
  num_heads: 8
  max_shortcut_steps: 6
  num_registers: 4
  action_space_type: "categorical"
  num_discrete_actions: 144

heads:
  input_dim: 256
  hidden_dim: 256
  num_layers: 2
  num_bins: 255
  num_actions: 144

training:
  phase1:
    steps: 10
    learning_rate: 3.0e-4
    weight_decay: 0.01
    warmup_steps: 2
    max_grad_norm: 1.0
    save_every: 10
    gradient_accumulation_steps: 2
    precision: "bfloat16"
    train_tokenizer: false

  phase2:
    epochs: 2
    max_steps: 20  # Stop after 20 steps for quick testing
    learning_rate: 1.0e-4
    weight_decay: 0.01
    warmup_steps: 5
    max_grad_norm: 1.0
    reward_weight: 1.0
    freeze_transformer: true
    save_every: 2
    save_every_steps: 10  # Save checkpoint every 10 steps
    use_focal_loss: true
    focal_gamma: 2.0
    focal_alpha: 0.25

  phase3:
    epochs: 2
    max_steps: 50  # Stop after 50 steps for quick testing
    batch_size: 4
    gradient_accumulation_steps: 4
    imagination_horizon: 15
    num_denoising_steps: 4
    discount: 0.997
    lambda: 0.95
    value_ema_decay: 0.999
    pmpo_alpha: 0.5
    pmpo_beta_kl: 0.3
    entropy_coef: 0.003
    advantage_bins: 16
    use_percentile_binning: true
    percentile_threshold: 10.0
    policy_lr: 3.0e-5
    value_lr: 1.0e-4
    value_loss_scale: 0.5
    grad_clip: 1.0
    save_every: 2
    num_workers: 4

optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: "cosine"
  min_lr: 1.0e-6

logging:
  log_every: 5
  eval_every: 1000
  use_wandb: false
  wandb_project: "dreamerv4_cosmos"

memory:
  gradient_checkpointing: false
  empty_cache_frequency: 100
