# DreamerV4 Configuration for MineRL Dataset - Small Batch Test
# Based on: "Training Agents Inside of Scalable World Models"

# Experiment settings
experiment:
  name: "dreamerv4_minerl_small"
  seed: 42
  device: "cuda"
  log_dir: "logs"
  checkpoint_dir: "checkpoints"

# Data settings
data:
  path: "data/mineRL_extracted"
  sequence_length: 16
  image_height: 64
  image_width: 64
  in_channels: 3
  batch_size: 4  # Small batch for testing
  num_workers: 2  # Reduced workers for small batch
  frame_skip: 1
  resize_to: null
  max_episodes: 50  # Limit to 50 episodes for quick testing

# Tokenizer settings (shared transformer architecture)
tokenizer:
  patch_size: 8
  embed_dim: 256
  latent_dim: 32
  num_latent_tokens: 16
  depth: 6
  num_heads: 8
  dropout: 0.0
  num_registers: 4
  mask_ratio: 0.75

# Dynamics model settings
dynamics:
  max_shortcut_steps: 6
  num_discrete_actions: 144
  action_space_type: "categorical"
  num_registers: 4

# Agent heads settings
heads:
  hidden_dim: 256
  num_layers: 2
  num_bins: 255
  num_actions: 144

# Training settings
training:
  # Phase 1: World Model Pretraining
  phase1:
    epochs: 5  # Fewer epochs for quick test
    learning_rate: 3.0e-4
    weight_decay: 0.01
    warmup_steps: 100  # Reduced for small test
    max_grad_norm: 1.0
    lpips_weight: 0.2
    save_every: 2  # Save more frequently for testing

  # Phase 2: Agent Finetuning
  phase2:
    epochs: 5  # Fewer epochs for quick test
    learning_rate: 1.0e-4
    weight_decay: 0.01
    warmup_steps: 50
    max_grad_norm: 1.0
    reward_weight: 1.0
    freeze_transformer: true
    save_every: 2
    use_focal_loss: true
    focal_gamma: 2.0
    focal_alpha: 0.25

  # Phase 3: Imagination Training with PMPO
  phase3:
    epochs: 5  # Fewer epochs for quick test
    batch_size: 4  # Small batch
    
    # Imagination rollout settings
    imagination_horizon: 15
    num_denoising_steps: 4
    
    # RL hyperparameters
    discount: 0.997
    lambda: 0.95
    
    # PMPO
    pmpo_alpha: 0.5
    pmpo_beta_kl: 0.3
    entropy_coef: 0.003
    advantage_bins: 16
    use_percentile_binning: true
    percentile_threshold: 10.0
    
    # Learning rates
    policy_lr: 3.0e-5
    value_lr: 1.0e-4
    
    # Loss weights
    value_loss_scale: 0.5
    
    # Training
    grad_clip: 1.0
    save_every: 2
    num_workers: 2

# Optimizer settings
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler settings
scheduler:
  type: "cosine"
  min_lr: 1.0e-6

# Logging settings
logging:
  log_every: 10  # Log more frequently for small test
  eval_every: 100
  use_wandb: false
  wandb_project: "dreamerv4"
