# DreamerV4 Configuration for MineRL Dataset - Small Subset for Testing
# Based on: "Training Agents Inside of Scalable World Models"
# This config is optimized for fast testing with a small subset (10-20 episodes)

# Experiment settings
experiment:
  name: "dreamerv4_minerl_subset"
  seed: 42
  device: "cuda"
  log_dir: "logs/subset"
  checkpoint_dir: "checkpoints/subset"

# Data settings
data:
  path: "data/mineRL_subset"  # Path to subset dataset
  sequence_length: 16
  image_height: 64
  image_width: 64
  in_channels: 3
  batch_size: 16  # Reduced for Phase 1 (trains both tokenizer + dynamics, needs more memory)
  num_workers: 4  # Workers for data loading
  frame_skip: 1
  resize_to: null
  max_episodes: null  # Use all episodes in subset

# Tokenizer settings (shared transformer architecture)
tokenizer:
  patch_size: 8
  embed_dim: 256  # Standard size for testing
  latent_dim: 32
  num_latent_tokens: 16
  depth: 6  # Standard depth
  num_heads: 8
  dropout: 0.0
  num_registers: 4
  mask_ratio: 0.75

# Dynamics model settings
dynamics:
  max_shortcut_steps: 6
  num_discrete_actions: 144
  action_space_type: "categorical"
  num_registers: 4

# Agent heads settings
heads:
  hidden_dim: 256
  num_layers: 2
  num_bins: 255
  num_actions: 144

# Training settings
training:
  # Phase 1: World Model Pretraining
  phase1:
    epochs: 5  # Increased from 1 - need more training for good reconstruction
    max_steps: null  # Remove step limit to train full epochs (was 100)
    learning_rate: 3.0e-4
    weight_decay: 0.01
    warmup_steps: 500  # Increased for better convergence
    max_grad_norm: 1.0
    lpips_weight: 0.2
    use_lpips: true  # Enable LPIPS for better visual quality (was false)
    save_every: 1  # Save every epoch
    save_every_steps: 200  # Save intermediate checkpoints every N steps

  # Phase 2: Agent Finetuning
  phase2:
    epochs: 2  # Very few epochs for quick test
    max_steps: 100  # Stop after 100 steps for quick test (overrides epochs if set)
    learning_rate: 1.0e-4
    weight_decay: 0.01
    warmup_steps: 100  # Increased for larger batch size
    max_grad_norm: 1.0
    reward_weight: 1.0
    freeze_transformer: true
    save_every: 1
    save_every_steps: 50  # Also save checkpoint every N steps
    use_focal_loss: true
    focal_gamma: 2.0
    focal_alpha: 0.25
    use_mtp: true
    mtp_length: 8

  # Phase 3: Imagination Training with PMPO
  phase3:
    epochs: 3  # Few epochs for quick test
    batch_size: 16  # Increased batch size to utilize GPU better (was 2)
    
    # Imagination rollout settings
    imagination_horizon: 15
    num_denoising_steps: 4
    
    # RL hyperparameters
    discount: 0.997
    lambda: 0.95
    
    # PMPO
    pmpo_alpha: 0.5
    pmpo_beta_kl: 0.3
    entropy_coef: 0.003
    advantage_bins: 16
    use_percentile_binning: true
    percentile_threshold: 10.0
    
    # Learning rates
    policy_lr: 3.0e-5
    value_lr: 1.0e-4
    
    # Loss weights
    value_loss_scale: 0.5
    
    # Training
    grad_clip: 1.0
    save_every: 1
    num_workers: 2  # Increased workers for faster data loading

# Optimizer settings
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Scheduler settings
scheduler:
  type: "cosine"
  min_lr: 1.0e-6

# Logging settings
logging:
  log_every: 10
  eval_every: 50
  use_wandb: false
  wandb_project: "dreamerv4"
