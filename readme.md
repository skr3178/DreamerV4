Dreamer V4 : code and implementation

## Overview

DreamerV4 is a scalable world model agent that learns complex control tasks (e.g., obtaining diamonds in Minecraft) **purely from offline video data**, without environment interaction. It combines a novel training objective with an efficient transformer architecture to enable **real-time, interactive, and accurate simulation**.

### Key Concepts

**Flow matching**: A formulation of diffusion models where the network predicts the velocity vector v = x‚ÇÅ - x‚ÇÄ that points towards clean data, rather than predicting noise. The signal level œÑ ‚àà [0,1] determines the mixture of noise and data, where œÑ=0 corresponds to pure noise and œÑ=1 means clean data.

**Shortcut models**: Help in getting to the right point effectively and fast by enabling coarser sampling steps. They allow the model to predict clean latents directly (x-prediction) rather than velocities, avoiding error accumulation in long rollouts.

**Diffusion forcing**: Extends flow matching to sequences by conditioning on past observations and actions, enabling action-conditioned video prediction.

---

# Algorithm 1: Training Phases

## Phase 1: World Model Pretraining
- **Tokenizer Training**: Train tokenizer on videos using masked autoencoding loss (equation 5).
- **Dynamics Model Training**: Train world model on tokenized videos and optionally actions using shortcut forcing objective (equation 7).
- **Objective**: Action-conditioned video prediction - learn to predict future frames given past frames and actions.

## Phase 2: Agent Finetuning
- **Task Conditioning**: Finetune world model with task inputs for policy and reward heads using equations (7) and (9).
- **Behavior Cloning**: Learn to predict actions and rewards from task-conditioned states.

## Phase 3: Imagination Training
- **Policy Optimization**: Optimize policy head using PMPO (equation 11) on imagined trajectories.
- **Value Learning**: Optimize value head using TD learning (equation 10) on trajectories generated by the world model and the policy head.
- **No Environment Interaction**: All rollouts are imagined inside the world model.

---

# Core Architecture

## Unified Transformer Architecture
- **Shared Architecture**: Tokenizer and dynamics model use the same block-causal transformer architecture and geometry.
- **Block-Causal Attention**: Attention is masked to be causal in time - tokens within a time step can attend to each other and all past time steps.
- **2D Efficient Transformer**: Handles both space and time dimensions efficiently.

---

# Dataset

## OpenAI VPT Dataset
- **Source**: Contractor gameplay data from Minecraft
- **Subsets**: 6-10 subsets
- **Total Duration**: 2,541 hours of gameplay
- **Image Resolution**: 360√ó640 pixels (high-resolution, reflecting human player experience)
- **Frame Rate**: 20 FPS
- **Context Length**: 192 frames (9.6 seconds at 20 FPS)

## Data Processing
- **Patchification**: Images are divided into 16√ó16 patches
- **Spatial Tokens**: 256 tokens per frame (after patchification)
- **Padding/Reshaping**: Needed to handle variable sequence lengths and ensure proper tensor shapes for batch processing

---

# Tokenizer

## Causal Tokenizer Architecture
- **Purpose**: Compresses raw video frames into continuous latent representations (tokens)
- **Input**: Partially masked image patches (16√ó16) + learned latent tokens
- **Architecture**: Block-causal transformer (same as dynamics model)
- **Bottleneck**: Projects latents to low-dimensional space with **tanh activation**
- **Output**: Continuous latent tokens z_t that represent compressed frame information

## Masked Autoencoding (MAE)
- **Masking Strategy**: Randomly drops up to 90% of patches during training
- **Objective**: Learn to reconstruct masked patches from visible patches and learned tokens
- **Loss Function**: 
  - **MSE Loss**: Mean squared error between predicted and ground truth patches
  - **LPIPS Loss**: Learned Perceptual Image Patch Similarity (weight: 0.2)
  - **Normalization**: Losses normalized via RMS (Root Mean Square)
  - **Total Loss**: `L_tokenizer = MSE + 0.2 √ó LPIPS` (normalized via RMS)

> Enables temporal compression while supporting frame-by-frame decoding for real-time interaction.

---

# Transformer Architecture

## Block-Causal Transformer
- **2D Efficient Design**: Handles both space and time dimensions
- **Attention Masking**: Causal in time dimension - tokens can attend to:
  - All tokens within the same time step (spatial attention)
  - All tokens from past time steps (temporal attention)
  - Cannot attend to future time steps

## Architecture Features
- **Pre-layer RMSNorm**: Normalization before each layer
- **RoPE (Rotary Position Embedding)**: Relative position encoding
- **SwiGLU**: Swish-Gated Linear Unit activation
- **QK-Norm**: Query-Key normalization for stability
- **Attention Logit Soft-Capping**: Prevents extreme attention weights

## Efficiency Optimizations
- **Flash Attention**: Memory-efficient attention computation
- **Factorized Attention**: 
  - Spatial-only attention layers (most layers)
  - Temporal attention every 4th layer
  - Reduces computational cost while maintaining expressiveness
- **GQA (Grouped-Query Attention)**: Reduces KV cache size for faster inference
- **Register Tokens**: Learned tokens that improve temporal consistency across frames

## Sequence Length Handling
- **Variable Length Training**: Swapping between long and short sequences during training
- **Length Generalization**: Can handle arbitrary sequence lengths at inference
- **Context Length**: 192 frames (9.6 seconds at 20 FPS)

---

# Loss Functions and Training Objectives

## Tokenizer Loss (Equation 5)
- **Masked Autoencoding Loss**: 
  - `L_tokenizer = MSE(predicted_patches, ground_truth_patches) + 0.2 √ó LPIPS(predicted_patches, ground_truth_patches)`
  - Normalized via RMS

## Dynamics Model Loss (Equation 7)
- **Shortcut Forcing Objective**: Combines diffusion forcing with shortcut models
- **x-prediction**: Predicts clean latents directly (not velocities) to avoid error accumulation
- **Bootstrap Loss**: Distills two small steps into one large step for coarser sampling
- **Ramp Loss Weight**: `w(œÑ) = 0.9œÑ + 0.1` - focuses learning on informative (less noisy) timesteps
- **Loss Components**:
  - Flow matching loss for sequences
  - Self-consistency loss for shortcut models

---

# DreamerV4 Core Equations (1-11)

## Equation 1: Flow Matching Loss
**Purpose**: Trains the network to predict the velocity that moves a noisy point toward clean data.

**Formulation**:
```
L_FM = E[||v_Œ∏(x_œÑ, œÑ) - (x‚ÇÅ - x‚ÇÄ)||¬≤]
```

Where:
- `v_Œ∏(x_œÑ, œÑ)`: Network prediction of velocity vector
- `x_œÑ = (1-œÑ)x‚ÇÄ + œÑx‚ÇÅ`: Interpolated point between noise x‚ÇÄ and clean data x‚ÇÅ
- `x‚ÇÄ ~ N(0, I)`: Pure noise
- `x‚ÇÅ`: Clean data
- `œÑ ‚àà [0,1]`: Signal level (œÑ=0: pure noise, œÑ=1: clean data)

‚Üí Core objective for learning smooth trajectories from noise to real data.

---

## Equation 2: Inference Sampling Step
**Purpose**: One step of ODE integration during generation.

**Formulation**:
```
x_{œÑ+ŒîœÑ} = x_œÑ + ŒîœÑ ¬∑ v_Œ∏(x_œÑ, œÑ)
```

Where:
- `ŒîœÑ`: Step size for ODE integration
- `v_Œ∏(x_œÑ, œÑ)`: Predicted velocity at current point

‚Üí How the model iteratively denoises starting from pure noise.

---

## Equation 3: Shortcut Models Bootstrap Loss
**Purpose**: For larger steps, predict the average of two half-steps (with stop-gradient on the intermediates).

**Formulation**:
```
v_target = (v_Œ∏(x_œÑ, œÑ, d/2) + v_Œ∏(x_œÑ + (d/2)¬∑v_Œ∏(x_œÑ, œÑ, d/2), œÑ + d/2, d/2)) / 2
L_bootstrap = ||v_Œ∏(x_œÑ, œÑ, d) - v_target||¬≤
```

Where:
- `d`: Large step size
- `d/2`: Half step size
- Stop-gradient applied to intermediate predictions

‚Üí Distills knowledge so the model can take big jumps without error accumulation.

---

## Equation 4: Shortcut Step Size & Signal Level Sampling
**Purpose**: Samples step sizes as powers of 2 and œÑ uniformly on the corresponding grid.

**Formulation**:
```
d ~ {2^k : k ‚àà {0, 1, ..., K}}
œÑ ~ Uniform([0, 1])
```

Where:
- `d`: Step size (powers of 2)
- `œÑ`: Signal level (uniformly sampled)
- `K`: Maximum step size exponent

‚Üí Ensures the model learns to handle variable (coarse/fine) sampling steps.

---

## Equation 5: Tokenizer Loss (Masked Autoencoding)
**Purpose**: MSE reconstruction + weighted LPIPS perceptual loss on patches.

**Formulation**:
```
L_tokenizer = MSE(·∫ë, z) + Œª_LPIPS ¬∑ LPIPS(·∫ë, z)
```

Where:
- `·∫ë`: Predicted patches
- `z`: Ground truth patches
- `Œª_LPIPS = 0.2`: Weight for LPIPS loss
- Losses normalized via RMS (Root Mean Square)

‚Üí Standard MAE objective to learn good visual representations.

---

## Equation 6: Shortcut Forcing Prediction (Informal)
**Purpose**: Predicts clean latent ·∫ë‚ÇÅ from noisy zÃÉ, conditioned on œÑ, d, actions.

**Formulation**:
```
·∫ë‚ÇÅ = f_Œ∏(zÃÉ, œÑ, d, a_{<t}, z_{<t})
```

Where:
- `zÃÉ`: Noisy latent at signal level œÑ
- `œÑ`: Signal level
- `d`: Step size
- `a_{<t}`: Past actions
- `z_{<t}`: Past latents

‚Üí Action-conditioned denoising step at the core of the dynamics model.

---

## Equation 7: Full Shortcut Forcing Loss
**Purpose**: MSE to clean data when d = d_min; otherwise weighted MSE to bootstrapped target (with ramp on (1-œÑ)¬≤).

**Formulation**:
```
L_shortcut = {
  ||·∫ë‚ÇÅ - z‚ÇÅ||¬≤                    if d = d_min
  w(œÑ) ¬∑ ||·∫ë‚ÇÅ - z_bootstrap||¬≤    otherwise
}
```

Where:
- `z_bootstrap`: Bootstrapped target from Equation 3
- `w(œÑ) = 0.9œÑ + 0.1`: Ramp loss weight (Equation 8)
- `d_min`: Minimum step size

‚Üí Prevents accumulation error in long imagined rollouts by using direct x-prediction + bootstrap.

---

## Equation 8: Ramp Loss Weight
**Purpose**: Higher weight when less noise (more informative).

**Formulation**:
```
w(œÑ) = 0.9œÑ + 0.1
```

Where:
- `œÑ ‚àà [0,1]`: Signal level
- Weight increases linearly with œÑ

‚Üí Focuses learning on cleaner, semantically richer timesteps.

---

## Equation 9: Behavior Cloning + Reward Prediction Loss
**Purpose**: Negative log-likelihood of future actions and rewards over 8 steps (multi-token prediction).

**Formulation**:
```
L_BC = -Œ£_{k=1}^{8} [log œÄ(a_{t+k}|s_{t+k}) + log p(r_{t+k}|s_{t+k})]
```

Where:
- `œÄ(a_{t+k}|s_{t+k})`: Policy probability of action a at step t+k
- `p(r_{t+k}|s_{t+k})`: Reward probability at step t+k
- `s_{t+k}`: Imagined states from world model

‚Üí Trains policy and reward heads via imitation + reward modeling.

---

## Equation 10: Value Head Loss (TD(Œª) Learning)
**Purpose**: Negative log-prob of Œª-returns (mix of bootstrapped value and n-step returns).

**Formulation**:
```
R_t^Œª = (1-Œª) Œ£_{n=1}^‚àû Œª^{n-1} R_t^{(n)}
L_value = -log p(R_t^Œª|s_t)
```

Where:
- `R_t^{(n)}`: n-step return
- `Œª`: TD(Œª) mixing parameter
- `R_t^Œª`: Œª-return (exponentially weighted average of n-step returns)

‚Üí Standard temporal-difference value learning for critic.

---

## Equation 11: PMPO Policy Loss
**Purpose**: Weighted log-prob on negative-advantage trajectories minus weighted log-prob on positive ones + KL to prior.

**Formulation**:
```
L_policy = (1-Œ±)/|D-| ¬∑ Œ£_{D-} log œÄ(a_i|s_i) 
         - Œ±/|D+| ¬∑ Œ£_{D+} log œÄ(a_i|s_i) 
         + Œ≤/N ¬∑ Œ£ KL[œÄ(a_i|s_i) || œÄ_prior]
```

Where:
- `A_t = R_t - v_t`: Advantage function
- `D+`: Trajectories with positive advantage (good actions)
- `D-`: Trajectories with negative advantage (bad actions)
- `Œ±`: Weight for positive advantage updates
- `Œ≤`: Weight for KL regularization
- `œÄ_prior`: Behavioral cloning prior (prevents policy from deviating too far)
- `N`: Total number of samples

‚Üí Proximal-style policy gradient that pushes good actions up and bad ones down, regularized.

---

# Dynamics Model

## Interactive Dynamics Model
- **Purpose**: Predicts future latent states given past latents and actions
- **Input Sequence**: Interleaved sequence of:
  1. **Action tokens** (a_t): Encoded keyboard and mouse actions
  2. **Signal level token** (œÑ): Single token representing noise level
  3. **Step size token** (d): Single token representing sampling step size
  4. **Latent tokens** (zÃÉ_t): Compressed frame representations

## Action Encoding
- **Keyboard Actions**: 23 binary variables (key press/release states)
- **Mouse Actions**: 121-class categorical distribution
  - Uses foveated Œº-law discretization
  - Captures mouse movement and clicks

## Token Composition
- **S_z**: Spatial tokens (256 per frame)
- **S_r**: Learned register tokens (improve temporal consistency)
- **Single token**: Shortcut signal level (œÑ) and step size (d)
- **S_a**: Action tokens (keyboard + mouse)
- **Total**: `SUM(S_z + S_r + Single_token + S_a)` = Learned embedding tokens

## Training
- **Action-Conditioned Prediction**: Learns to predict future frames given past frames and actions
- **Shortcut Forcing**: Enables fast, accurate generation with few sampling steps (K=4)
- **Inference Speed**: 21 FPS on single H100 GPU

---

# Reinforcement Learning

## PMPO (Policy Mirror Proximal Policy Optimization)
- **Purpose**: Robust policy gradient method using advantage signs
- **Advantage Function**: `A_t = R_t - v_t`
  - R_t: Return (cumulative discounted reward)
  - v_t: Value estimate

## Policy Head Components
- **Input**: Imagined states z = {z_t} from world model
- **Output**: Action distributions a = {a_t}
- **Architecture**: MLP heads over 8-step MTP (Multi-Task Prompt)

## Value Head Components
- **Input**: Imagined states s_i from world model
- **Output**: Value estimates v = {V_t}
- **Architecture**: MLP heads

## Reward Head Components
- **Input**: Imagined states s_i from world model
- **Output**: Reward predictions r = {r_t}
- **Architecture**: MLP heads

## PMPO Loss (Equation 11)
- **Advantage Binning**: Sort trajectories into bins based on advantage magnitude:
  - **D+**: Positive advantage trajectories (good actions)
  - **D-**: Negative advantage trajectories (bad actions)
- **Loss Formula**:
  ```
  L_policy = (1-Œ±)/|D-| √ó Œ£_{D-} ln œÄ(a_i|s_i) 
           - Œ±/|D+| √ó Œ£_{D+} ln œÄ(a_i|s_i) 
           + Œ≤/N √ó Œ£ KL[œÄ(a_i|s_i) || œÄ_prior]
  ```
  - Œ±: Weight for positive advantage updates
  - Œ≤: Weight for KL regularization
  - œÄ_prior: Behavioral cloning prior (prevents policy from deviating too far)

## Training Strategy
- **Frozen Transformer**: World model transformer remains frozen during RL training
- **Only Heads Train**: Only policy and value heads are optimized
- **Imagination-Only**: All rollouts happen inside the world model, no environment interaction

---

# Flow Matching and Shortcut Models

## Flow Matching Basics
- **Formulation**: Network predicts velocity vector v = x‚ÇÅ - x‚ÇÄ pointing towards clean data
- **Interpolation**: x_œÑ = (1-œÑ)x‚ÇÄ + œÑx‚ÇÅ
  - x‚ÇÄ ~ N(0, I): Pure noise
  - x‚ÇÅ: Clean data
  - œÑ ‚àà [0,1]: Signal level

## Shortcut Forcing
- **Combines**: Diffusion forcing (for sequences) + shortcut models (for fast sampling)
- **x-prediction**: Predicts clean latents directly instead of velocities
  - Avoids error accumulation in long rollouts
  - More stable for multi-step generation
- **Bootstrap Loss**: Distills two small steps into one large step
  - Enables coarser sampling (fewer steps needed)
  - Maintains quality with fewer forward passes
- **Ramp Loss Weight**: `w(œÑ) = 0.9œÑ + 0.1`
  - Focuses learning on informative (less noisy) timesteps
  - Reduces emphasis on very noisy timesteps (œÑ ‚âà 0)

## Inference
- **Sampling Steps**: K=4 steps (very few compared to standard diffusion)
- **Speed**: 21 FPS on single H100 GPU
- **Real-time**: Enables interactive generation and imagination training


# DreamerV4 Architecture Summary

DreamerV4 is a scalable world model agent that learns complex control tasks (e.g., obtaining diamonds in Minecraft) **purely from offline video data**, without environment interaction. It combines a novel training objective with an efficient transformer architecture to enable **real-time, interactive, and accurate simulation**.

---



### 1. **Causal Tokenizer**
- **Purpose**: Compresses raw video frames into continuous latent representations.
- **Input**: Partially masked image patches (16√ó16) + learned latent tokens.
- **Architecture**: Block-causal transformer (same as dynamics model).
- **Bottleneck**: Projects latents to low-dimensional space with **tanh activation**.
- **Training Objective**: 
  - **Masked Autoencoding (MAE)**: Randomly drops up to 90% of patches.
  - **Loss**: `MSE + 0.2 √ó LPIPS` (normalized via RMS).

> Enables temporal compression while supporting frame-by-frame decoding for real-time interaction.

---

### 2. **Interactive Dynamics Model**
- **Purpose**: Predicts future latent states given past latents and actions.
- **Input Sequence**: Interleaved `[action_t, signal_level_œÑ, step_size_d, zÃÉ_t]`.
- **Actions**: 
  - Keyboard: 23 binary variables.
  - Mouse: 121-class categorical (foveated Œº-law discretization).
- **Architecture**: Efficient block-causal transformer with:
  - **Factorized attention**: Spatial-only layers + temporal attention every 4th layer.
  - **GQA (Grouped-Query Attention)**: Reduces KV cache size.
  - **Register tokens**: Improve temporal consistency.
  - **RoPE, RMSNorm, SwiGLU, QK-Norm, logit soft-capping**.

---

### ‚ö° Training Objective: **Shortcut Forcing**

Combines **diffusion forcing** (for sequences) + **shortcut models** (for fast sampling):

- **x-prediction**: Predicts clean latents directly (not velocities) ‚Üí avoids error accumulation in long rollouts.
- **Bootstrap loss**: Distills two small steps into one large step for coarser sampling.
- **Ramp loss weight**: `w(œÑ) = 0.9œÑ + 0.1` ‚Üí focuses learning on informative (less noisy) timesteps.
- **Inference**: Generates at **K=4 sampling steps** ‚Üí **21 FPS on H100**.

---

### ü§ñ Agent Integration

After pretraining, the world model is extended into a full agent:

1. **Task Conditioning**: Insert task tokens into transformer.
2. **Behavior Cloning**: Predict actions/rewards via MLP heads over 8-step MTP.
3. **Imagination RL**: 
   - Uses **PMPO** (robust policy gradient using advantage signs).
   - Trains only policy/value heads; transformer remains frozen.
   - No environment interaction ‚Äî all rollouts are imagined.


---

### ‚úÖ Key Innovations

- First agent to **obtain diamonds in Minecraft from offline data only**.
- Learns action grounding from **<100 hrs of labeled actions** + 2,500 hrs unlabeled video.
- Unified architecture for **tokenization + dynamics**.
- **Shortcut forcing** enables high-quality, real-time interactive generation.
- Outperforms prior world models (Oasis, Lucid, MineWorld) in accuracy and interactivity.

---



- Use custom tokenizer and the dynamics as the same transformer architecture and geometry. 
- use block causal tranformers 
- PMPO for RL (current MPO code redesign for PMPO)
- Tokenizer (masked autoencoding)
- Loss term (MSE and LIPS)

Input to dynamics model (interleaved): 

1. Actions (mouse and keyboard dataset/need to find an alternative game containing this)
2. Observations

S_z : spatial token
S_r : Learned register tokens
Single token : Shortcut signal level and step size
S_a : Action token

SUM (Sz+Sr+Single_token+Sa) : Learned embedding token

# training phases
- Phase 1:
Pre-trained world model on action conditioned video prediction

- Phase 2:
Learn policy and reward model


RL training phase
Policy head parts
[
- use z = {z_t} flow heads
- a = {a_t} action heads
]

reward head parts
[
- use r = {r_t} reward heads
- use v = {V-t} value heads
- s_i = imagined states
]

Advantage At = Rt- vt
Policy heads learns using PMPO

Depending upon the advantage magnitude, sort into bins of:
[D+ or D-] for loss computation as below:

L = (1-a)/|D-| * Summ ln pi(ai|si) - a/|D+|summ ln pi(ai|si) + b/N * summ KL[pi(ai|si) ||pi_prior]


---

# Implementation Details

## Inputs
- **Images**: High-resolution frames (360√ó640) from Minecraft gameplay
- **Actions**: 
  - Keyboard: 23 binary variables (key states)
  - Mouse: 121-class categorical (foveated Œº-law discretization)
- **Tasks**: Task tokens for multi-task learning (optional in Phase 1, required in Phase 2+)

## Outputs
- **Latent Tokens**: Compressed frame representations (z_t)
- **Actions**: Predicted action distributions (for behavior cloning)
- **Rewards**: Predicted reward signals (r_t)
- **Values**: Value estimates (V_t) for RL

## Task Sets
- **Minecraft Tasks**: 20 different tasks (see Table 4 in paper):
  1. mine_log, mine_cobblestone, mine_iron_ore, mine_coal, mine_diamond
  2. craft_planks, craft_stick, craft_crafting_table, craft_furnace, craft_iron_ingot
  3. craft_wooden_pickaxe, craft_stone_pickaxe, craft_iron_pickaxe
  4. open_crafting_table, open_furnace
  5. place_crafting_table, place_furnace
  6. use_wooden_pickaxe, use_stone_pickaxe, use_iron_pickaxe

- **Evaluation Prompt Sequence**: Ordered sequence of tasks leading to diamond collection (see Table 6 in paper)
- **Milestone Items**: Progress tracking items (see Table 5 in paper)

## Action Space
- **Keyboard**: 23 binary distributions (independent Bernoulli)
- **Mouse**: 121-class categorical distribution
- **Total Action Space**: 23 binary + 1 categorical (121 classes)

---



# Key Technical Details

## Patchification and Tokenization
- **Patch Size**: 16√ó16 pixels
- **Patches per Frame**: (360/16) √ó (640/16) = 22.5 √ó 40 = 900 patches
- **Spatial Tokens**: 256 tokens per frame (compressed from 900 patches)
- **Compression Ratio**: ~3.5√ó spatial compression

## Sequence Interleaving
The dynamics model processes an interleaved sequence:
```
[action_t, œÑ, d, zÃÉ_t, action_{t+1}, œÑ, d, zÃÉ_{t+1}, ...]
```
- Each time step includes: action, signal level, step size, and latent tokens
- This allows the model to condition generation on actions and control sampling

## Register Tokens
- **Purpose**: Improve temporal consistency across frames
- **Number**: Learned during training (typically 8-16 tokens)
- **Function**: Act as "memory" tokens that help maintain coherence in long sequences

## Multi-Task Prompting (MTP)
- **8-step MTP**: Policy and reward heads use 8-step lookahead
- **Task Conditioning**: Task tokens inserted into transformer sequence
- **Generalization**: Model learns to handle multiple tasks from single architecture

---

# Evaluation Metrics

## Success Rates
- **Milestone Items**: Track progress through crafting tree (log ‚Üí planks ‚Üí crafting table ‚Üí ... ‚Üí diamond)
- **Evaluation Episodes**: 1000 episodes per milestone
- **Success Threshold**: Item obtained within episode

## Time to Milestone
- **Average Time**: Minutes needed to reach each milestone (for successful episodes)
- **Efficiency Metric**: Measures how quickly agent can progress through task sequence

## Comparison Baselines
- VPT (pretrained and finetuned)
- BC (Behavioral Cloning)
- WM+BC (World Model + Behavioral Cloning)
- VLA (Vision-Language-Action) with Gemma 3
- Dreamer 4 (this work)

---

# Implementation Checklist

## Phase 1: World Model Pretraining
- [ ] Implement causal tokenizer with masked autoencoding
- [ ] Implement block-causal transformer architecture
- [ ] Implement flow matching / shortcut forcing objective
- [ ] Train on unlabeled video data (2,500+ hours)
- [ ] Train on labeled action-video pairs (<100 hours)

## Phase 2: Agent Finetuning
- [ ] Add task conditioning tokens
- [ ] Implement policy head (MLP over 8-step MTP)
- [ ] Implement reward head (MLP)
- [ ] Implement value head (MLP)
- [ ] Finetune with task-conditioned data

## Phase 3: Imagination Training
- [ ] Implement PMPO algorithm
- [ ] Implement advantage computation and binning
- [ ] Implement policy loss with KL regularization
- [ ] Implement value learning (TD learning)
- [ ] Run imagination rollouts (no environment interaction)

## Infrastructure
- [ ] Data loading pipeline for VPT dataset
- [ ] Patchification and tokenization utilities
- [ ] Sequence interleaving for dynamics model
- [ ] Efficient attention implementation (Flash Attention)
- [ ] GQA implementation for reduced KV cache
- [ ] Distributed training setup (FSDP)

---

# Notes and Questions

## Open Questions
- **Alternative Datasets**: Need to find alternative games with mouse/keyboard actions if VPT dataset is not available
- **Shared Attention**: Can we use shared attention (spatial and temporal) since it can learn from each other with proper masking?
- **Patchification Details**: Why exactly is padding/reshaping needed? (Answer: To handle variable sequence lengths and ensure proper tensor shapes for batch processing)

## Implementation Considerations
- **Memory Efficiency**: Use gradient checkpointing for large models
- **Mixed Precision**: Use FP16/BF16 for training speed
- **Sequence Length**: Implement variable-length batching for efficiency
- **KV Cache**: Use GQA to reduce memory footprint during inference





‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DREAMERV4 TRAINING PIPELINE                          ‚îÇ
‚îÇ                    (Identical for ALL datasets)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  PHASE 1    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PHASE 2    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PHASE 3    ‚îÇ
    ‚îÇ             ‚îÇ         ‚îÇ             ‚îÇ         ‚îÇ             ‚îÇ
    ‚îÇ  Pretrain   ‚îÇ         ‚îÇ  Finetune   ‚îÇ         ‚îÇ Imagination ‚îÇ
    ‚îÇ  World      ‚îÇ         ‚îÇ  Agent      ‚îÇ         ‚îÇ    RL       ‚îÇ
    ‚îÇ  Model      ‚îÇ         ‚îÇ  Heads      ‚îÇ         ‚îÇ   (PMPO)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                       ‚îÇ                       ‚îÇ
          ‚ñº                       ‚ñº                       ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Tokenizer + ‚îÇ         ‚îÇ Behavior    ‚îÇ         ‚îÇ Policy +    ‚îÇ
    ‚îÇ Dynamics    ‚îÇ         ‚îÇ Cloning     ‚îÇ         ‚îÇ Value       ‚îÇ
    ‚îÇ Transformer ‚îÇ         ‚îÇ + Reward    ‚îÇ         ‚îÇ Optimization‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                       ‚îÇ                       ‚îÇ
          ‚ñº                       ‚ñº                       ‚ñº
    Loss: Eq.5,7              Loss: Eq.9              Loss: Eq.10,11
    (MAE, Shortcut)           (BC + Reward)           (TD + PMPO)




‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         DREAMERV4 - UNIVERSAL ARCHITECTURE                      ‚îÇ
‚îÇ                        (Same for MinAtar, NH, SOAR, VPT)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PHASE 1: WORLD MODEL PRETRAINING (All datasets use this)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Observations ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     TOKENIZER        ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Latent z   ‚îÇ
     ‚îÇ (varies by   ‚îÇ         ‚îÇ  (encoder/decoder)   ‚îÇ         ‚îÇ   Tokens     ‚îÇ
     ‚îÇ  dataset)    ‚îÇ         ‚îÇ  Loss: MAE + LPIPS   ‚îÇ         ‚îÇ              ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                      ‚îÇ
                                                                      ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ   Actions    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  DYNAMICS MODEL      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Predicted    ‚îÇ
     ‚îÇ   a_t        ‚îÇ         ‚îÇ  (Block-Causal       ‚îÇ         ‚îÇ Future z     ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ   TRANSFORMER)       ‚îÇ         ‚îÇ              ‚îÇ
                              ‚îÇ  Loss: Shortcut      ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ        Forcing       ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚îÇ YES - ALL USE TRANSFORMER
                                       ‚ñº

PHASE 2: AGENT FINETUNING (All datasets use this)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                    FROZEN TRANSFORMER                            ‚îÇ
     ‚îÇ                                                                  ‚îÇ
     ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
     ‚îÇ   ‚îÇ Policy Head ‚îÇ    ‚îÇ Reward Head ‚îÇ    ‚îÇ Value Head  ‚îÇ          ‚îÇ
     ‚îÇ   ‚îÇ   œÄ(a|s)    ‚îÇ    ‚îÇ   r(s)      ‚îÇ    ‚îÇ   V(s)      ‚îÇ          ‚îÇ
     ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
     ‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                  ‚îÇ
     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
     ‚îÇ                            ‚ñº                                     ‚îÇ
     ‚îÇ                  Behavior Cloning Loss                           ‚îÇ
     ‚îÇ                  (Eq. 9 in paper)                                ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


PHASE 3: IMAGINATION RL (All datasets use this)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                      IMAGINATION ROLLOUTS                       ‚îÇ
     ‚îÇ                                                                 ‚îÇ
     ‚îÇ   z_0 ‚îÄ‚îÄ‚ñ∂ [World Model] ‚îÄ‚îÄ‚ñ∂ z_1 ‚îÄ‚îÄ‚ñ∂ z_2 ‚îÄ‚îÄ‚ñ∂ ... ‚îÄ‚îÄ‚ñ∂ z_H         ‚îÇ
     ‚îÇ            (frozen)           ‚îÇ       ‚îÇ              ‚îÇ          ‚îÇ
     ‚îÇ                               ‚ñº       ‚ñº              ‚ñº          ‚îÇ
     ‚îÇ                             r_1     r_2            r_H          ‚îÇ
     ‚îÇ                                                                 ‚îÇ
     ‚îÇ   Policy Loss: PMPO (Eq. 11)                                    ‚îÇ
     ‚îÇ   Value Loss: TD(Œª) (Eq. 10)                                    ‚îÇ
     ‚îÇ                                                                 ‚îÇ
     ‚îÇ   NO ENVIRONMENT INTERACTION - ALL IN IMAGINATION               ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                    DREAMERV4 ARCHITECTURE               ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                                   ‚îÇ                                   ‚îÇ
          ‚ñº                                   ‚ñº                                   ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   MinAtar   ‚îÇ                    ‚îÇ NH Dataset  ‚îÇ                    ‚îÇ     VPT     ‚îÇ
   ‚îÇ  (Simple)   ‚îÇ                    ‚îÇ  (Medium)   ‚îÇ                    ‚îÇ   (Full)    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                                   ‚îÇ                                   ‚îÇ
          ‚ñº                                   ‚ñº                                   ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Flatten+MLP ‚îÇ                    ‚îÇ CNN/MLP or  ‚îÇ                    ‚îÇ ViT-style   ‚îÇ
   ‚îÇ (no MAE)    ‚îÇ                    ‚îÇ VideoToken  ‚îÇ                    ‚îÇ VideoToken  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                                   ‚îÇ                                   ‚îÇ
          ‚ñº                                   ‚ñº                                   ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Small       ‚îÇ                    ‚îÇ Medium      ‚îÇ                    ‚îÇ Large       ‚îÇ
   ‚îÇ Transformer ‚îÇ                    ‚îÇ Transformer ‚îÇ                    ‚îÇ Transformer ‚îÇ
   ‚îÇ (4 layers)  ‚îÇ                    ‚îÇ (8 layers)  ‚îÇ                    ‚îÇ (24 layers) ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                                   ‚îÇ                                   ‚îÇ
          ‚ñº                                   ‚ñº                                   ‚ñº
   Actions: 6                         Actions: 16                        Actions: 144
   discrete                           continuous                         discrete